---
title: "Computational Lab Notebook"
author: "Tess"
date: "9/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sept 23, 2020

*Goal:* Plot DBH's from 2016 across all sites in diameters_clean

*Worked* 
ggplot(data = diameters_clean, mapping = aes(x = spp, y = year_2016)) + geom_boxplot() + labs(title = "2016 Diameters by Species", x = "Species" , y = "2016 Diameter (DBH)") + theme_bw() +theme(text = element_text(size = 20)) + theme(axis.text.x = element_text(angle = 90, size= 16))

*Did not work*
ggplot(data = diameters_clean, mapping = aes(x = site, y = year_2016)) + geom_boxplot() + labs(title = "2016 Diameters by Site", x = "Site" , y = "2016 Diameter (DBH)") + theme_bw() +theme(text = element_text(size = 20))
Warning message:
Continuous x aesthetic -- did you forget aes(group=...)? 

also

ggplot(data = diameters_clean, mapping = aes(x = site, y = year_2016, group = spp)) + geom_bar() + labs(title = "2016 Diameters by Site", x = "Site" , y = "2016 Diameter (DBH)") + theme_bw() +theme(text = element_text(size = 20))
Error: stat_count() can only have an x or y aesthetic.

## Sept. 28, 2020
Bergman's rule relates to mammal body sizes and temps - develop a hypothesis about what we would expect to happen to mammalian body sizes if Bergmans rule is true? are there enough data to test this hypothesis in the surveys_complete? what additional data would we like/need?

*Hypothesis* Mammalian size will decrease as climate change continues to warm the earth

*Data wanted/needed* Need climate data, need mammal dimensions


install notes - can be messy and mess things up or install incompletely
install NOAA data, need to go to website to get API key in order to download large amounts of data
paste noaa key into .rprofile found in project file

  print.cache_info httr
 means that code from pkg overrides base r code

lawn_bbox_polygon(c(-114, 32, -115, 31)) %>% view()
code allows you to pull up a chunk of the map by lat and long, use google maps to get lat and long for a place we haven't been - long first then lat

temp_data <- meteo_tidy_ghcnd(stationid = "USW00003145", var = c("tmin", "tmax"))
file min/max dates: 1982-01-01 / 2020-09-30

## September 30, 2020
lm used for linear model in R
```{r}
lm(date~tmin, calc_f)
```

Call:
lm(formula = date ~ tmin, data = calc_f)

Coefficients:
(Intercept)         tmin  
  11083.123        1.865  
  
  -provides intercept and slope, date is generally with tmin, can use this to plot
 
```{r}
ggplot (calc_f, mapping = aes(x = date, y = tmin)) +geom_point() + geom_smooth(method= "lm")
```

now insert lm into geom_smooth to plot trend, don't have to program it previously, can do it in geom_smooth
ANOVA testing in R, will show us popular tests and related ones at later date
```{r}
new_surveys <- surveys %>% 
+ mutate(date = make_date(year, month, day))
```

update surveys complete to have consistent date, make_date is a lubridate (library) function
save updated file

```{r}
write_csv(new_surveys, "data_output/surveys_date.csv")
```


merge new_surveys and calc_f, merges by matching columns, will merge those rows that have matching columns, and this will be done by date as that is the matching column, we are doing an innerjoin, rows that are not matching are ommitted

```{r}
merge(calc_f, new_surveys)
```

now plot
```{r}
ggplot(temp_surveys, mapping = aes(x = tmin, y = weight)) + geom_point(alpha = .1) +geom_smooth(method = "lm")
```
doesn't look good or correct



## October 5, 2020
-revision management and data manipulation
-signed up for github tesseracrockett (username)
-logged into repository for our class, repository is a place for code of a project, all revisions go here
-commits do = revisions/edits
-fork the code = this makes a copy of the entire repository and saves to personal username, we now have copies of everthing under our usernames on github
-learning to back up changes in R studio to get hub
-move to terminal from console, it's more archaic, allows us to access file system of comp, directly access commands for comp that operating systems typically mask
-use to tell revision management system who we are, in terminal type following to title ourselves for revision and tell it who we are 
rstudio-user@application-2678354-deployment-6956621-jhbrx:/cloud/project$ git config --global user.name "Tess Crockett"
rstudio-user@application-2678354-deployment-6956621-jhbrx:/cloud/project$ git config --global user.name "tessera.crockett@selu.edu"
-adding code to repository 
  workflow = add, commit, push 
    add = tell git what file we've edited/added to
    commit = tell git to save that file, we're committing to the work we've done
    push = put the code in the online repository
-each commit has it's own unique code - on git, anything with green background is new code that has been added, red means it's been deleted
- we have some initial code, we mess with it, then we have code version two, mess with it, code version 3 - this is a bad habit which is dangerous workflow - in git you have a managed workflow, your file name is overwritten and does not change, but you have your initial commit with it's own unique id, and each subsequent commit has it's own as well - you have one file in R,
but all previous loads in git 

##October 7, 2020
commit and push practice, learn to pull 

##October 12, 2020
linear models

predictor variable = independent variable
response variable = dependent variable

linear model implies a statistical framework for quanitfying to what degree one or more predictor variable describes the variation seen in a response variable - see linear models page in compbio webpage

crab data
body depth is response variable, expect it to be predicted by others
start with simple linear regression with single numeric predictor, assumption 1 is fulfilled - this is linear (see below code)
```{r}
ggplot(crabs, mapping = aes (x= carapace_length, y=body_depth )) + geom_point()
```

build linear model - first response variable, (which is predicted by) then predictor, then data set
```{r}
model_fit <- lm (body_depth ~ carapace_length, data = crabs)
summary(model_fit)
```
gives us:

  Call:
lm(formula = body_depth ~ carapace_length, data = crabs)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.96587 -0.47864  0.07071  0.49976  1.43543 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     -1.15527    0.20517  -5.631 6.09e-08 ***
carapace_length  0.47300    0.00624  75.803  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.6266 on 198 degrees of freedom
Multiple R-squared:  0.9667,	Adjusted R-squared:  0.9665 
F-statistic:  5746 on 1 and 198 DF,  p-value: < 2.2e-16

  residuals look normal, coefficients is where we learn our data values. intercept is expected mew of beta naught (y), basic t test to see if intercept is anything but zero = -1.155 +/- 0.205
  don't over-interpret the slope or intercept is biologically meaningful - i.e. a body depth of -1.155 makes no sense
  estimate of carapace length is fitted value for slope, expected change to response variable as the predictor variable increases 
  p value is fairly significant, it is different than 0
  *** means more significant than it can show you
  we want residual standard area to be close to zero
  97% of body depth can be explained by carapace length (this is what adjusted r squared tells us)
  plot the model:
```{r}
ggplot(crabs, mapping =aes(x=carapace_length, y=body_depth))+ geom_point(size = 0.5) +geom_smooth(method = "lm", color ="navy", fill = "deeppink4") + annotate( "text", x = 20, y = 30, label = "R^2 == 0.966") + theme_bw()
```
  annotate allows us to add text to places on graph - need x and y points to tell it where to place text
  April says that confidence intervals tell us the error on our estimate, related to standard error of estimate (se = false will turn off standard error, it is on by default)
  look at residuals, model fit just gives us summary stats
  use broom to pull residuals out of model fit object, use augment to pull up all data computed in process of computing best fit model
```{r}
broom::augment(model_fit)
```
save residuals to a variable 
```{r}
augmented_fit <- broom::augment(model_fit)
```
make a qq plot 
```{r}
qqnorm(augmented_fit$.resid)
qqline(augmented_fit$.resid, color = "red")
```
add fit line, want residuals close to line 
  
  now do simple ANOVA - a linear model with a categorical variable
```{r}
model_fit <-lm(body_depth ~ sex, crabs)
```
view 
```{r}
summary(model_fit)
```
Call:
lm(formula = body_depth ~ sex, data = crabs)

Residuals:
   Min     1Q Median     3Q    Max 
-7.624 -2.449  0.076  2.463  7.376 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  13.7240     0.3420  40.134   <2e-16 ***
sexM          0.6130     0.4836   1.268    0.206    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.42 on 198 degrees of freedom
Multiple R-squared:  0.00805,	Adjusted R-squared:  0.00304 
F-statistic: 1.607 on 1 and 198 DF,  p-value: 0.2064

  the average male crab is going to have body depth of 0.613 larger than female
  p value is not significant
  
run simple anova - takes model fit object and converts to anova
```{r}
anova_model_fit <- aov(model_fit)

summary(anova_model_fit)
```
gives us:
         Df      Sum Sq  Mean Sq   F value   Pr(>F)
sex           1   18.8   18.79   1.607  0.206
Residuals   198 2315.3   11.69       
  anova should tell same thing as linear model

  
  
  

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
